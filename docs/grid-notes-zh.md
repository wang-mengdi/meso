# 网格系统

我们的网格系统实现于`Grid<d>`、`Field<T,d,side>`、`FaceField<T,d,side>`三个类型。其中，`Grid<d>`并不拥有数据，是一个为方便计算起见而提取出的接口，剩下两个类型拥有数据。

# Grid类型

我们对`Grid<d>`类型的设计是：**Grid<d>本身代表且仅代表“一些晶格数据点（lattice data points）在内存中的排布”。** 下文中我们会解释这句话。在代码和文档中，我们把这些晶格数据点称为node，下文中亦使用这一术语。

`Grid<d>`类型负责如下三种任务：

- 计算地址/下标映射、node位置、网格尺寸等基础数据。对于地址映射，我们按照SPGrid的思路，三维按$4\times 4\times 4$切块，二维按$8\times 8$切块，这样每一个块内含有64个元素，每块内的元素在内存当中相邻，这样适配CUDA的缓存大小，访存较为友好。
- 执行计算node邻居、提取face grid、cell grid等几何相关运算。
- 遵循编程规范中`iterator-lambda`项的要求，提供并行/串行遍历全部nodes的iterator接口。

接下来我们解释`Grid<d>`类型的设计思路。在计算流体力学中，会用到两种网格，一种是同位网格/Collocation Grid，另一种是交错网格/Staggered Grid/MAC Grid.其中，同位网格只有数据点的概念，也就是我们所说的“晶格数据点”；而交错网格有格子(cell)和面(face)的概念，其含义是，把计算域划分成$\Delta x^3$的格子，压强这样的标量储存在格子的中心（cell center），速度这样的矢量储存在面的中心（face center）。

那么对于同位网格，我们可以把数据点中间$\Delta x^3$的空间定义为格子/cell，那么数据点位于格子的顶点。总结一下，就是在网格系统中有三种可能存储数据的地方：格子中心（cell center）、格子的顶点（cell corner）、面的中心（face center）。

在`Meso`中，我们作如下设定：这三种数据位置在**编程**上是相同的，它们之间的区别只是一种**解释**上的区别。例如，考虑一个$8\times 8\times 8$的交错网格，它的格子中心一共有$8^3$个数据点，而一个$8\times 8\times 8$的同位网格也有$8^3$个数据点。这两组数据点在内存中的排列顺序完全相同，进而，如果它们的$\Delta x$（也就是`Grid<d>::dx`）相同，而坐标最小的数据点（也就是`Grid<d>::pos_min`）相同，那么这两组数据点在空间中的位置亦完全相同。事实上，它们所对应的`Grid<d>`也是完全相同的。

但是在物理解释上，这两种情况之间存在区别。即，



从数据结构的角度而言，这两者其实是一样的，区别仅仅在于它们与物理世界的对应不同，以及前者隐含着存在一种cell网格-face网格之间的转换。但就程序而言，它们并不影响数据的排列方式。因此，我们在enum class GridType中定义MAC和COLLOC两种模式，分别对应MAC网格和Collocation网格，以供初始化时分辨。所有和FACE相关的

这样做会带来一个小问题，即我们的程序无法知道一个Grid所对应的“真正”的计算域，但这并不重要，因为计算域是一个物理信息，那么物理信息和数据结构的对应关系应由模拟器维护，而不是数据结构。

这样做的方便之处是，我们可以轻松地把MAC网格某一轴向上的面上的量视作另一个Grid，如此便可统一处理。

在CPX的设计中，Face_Index不同轴向所对应的编码方式不同。为简便起见，我们暂时忽略这种不同，一律grid的方式编码，此时块内按照z-y-x的顺序编码（即，改z对应的index改变量最大，y次之，x最小），这相当于CPX中的axis==d-1的方式编码，即二维的axis=1，三维的axis=2。若今后想要重新恢复这种编码方式的区分，可以通过给Grid类再加一个模板参数的方法实现。

# 网格系统的储存

我们保证，FaceField中每个轴向数据的储存顺序严格等同于grid使用Face_Grid()生成的CORNER模式grid的储存顺序。

Field和FaceField均按照“std::vector”的直觉设计，即所有的默认拷贝都是深拷贝，这样让用户不必操心未初始化指针的问题，可以放心复制。但实际上，其中的数据以std::shared_ptr形式存储，如此设计的原因是，可以临时地把FaceField在某个轴向上的数据取出，作为一个Field，执行Field专有的操作。
