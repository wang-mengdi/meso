# 面向2035的先进物理模拟器开发方案

## 综述

随着计算机运算能力的不断提升，物理模拟领域正在日新月异地演进，让人们逐步拥有了使用计算机模拟更多种类、更大规模、更高精度的的物理系统的能力。在这种新形势下，2020年代的物理模拟领域对我们使用的工具——也就是SimpleX代码库提出了更高的要求。具体来说，我们认为未来的这一领域将呈现出如下趋势：

- **多学科、多领域走向融合。** 当前，计算机技术已经成为科研和工程开发中全平台、全领域的基础工具和基本手段，成为21世纪的“微积分”。尤其在人工智能浪潮的助推下，计算机技术正在以前所未有的速度向所有学科渗透，在这种趋势下，计算机图形学、CFD、科学计算等领域正在逐渐走向交叉融合，共享相同的基础工具和方法论。一方面，随着通用GPU带来的计算能力革命，个人计算机正在逐渐拥有传统意义上CFD级别的求解能力，传统意义上的CG研究者也可以向工程和自然科学领域进发，在3D打印、机器人控制、公共卫生等领域大显身手；另一方面，诸如磁流体动力、等离子体、高马赫数流体、激波求解之类已经被其他学科充分研究，但在计算机学科尚属新奇的物理现象和物理系统，也可能在未来成为CG领域探索的主题。这要求我们的代码向CFD学习，拥有更高的精度、更快的速度、更正规的实验方式，以适应学科融合的现实需要。
- **多尺度、多规律频繁交叉。** 在过去，对“一般情况”的处理已经被充分研究，很难继续构成图形学的研究前沿。相反，图形学的最新进展往往集中在各种尺度、规律、特性交叉的“边缘情况”或“极端情况”。例如，固体和流体的交叉——MPM；面元模拟和体元模拟的交叉——IPC；宏观尺度和微观尺度的交叉——肥皂泡等等。这就要求我们的代码既足够复杂，以包含多种规律、多种尺度，又不被这种复杂所困，能简单有效，甚至自动地处理它们。
- **多手段、多方法综合运用。** “边缘情况”本身的特点决定，对它的研究离不开对多种数学手段，多种计算方法的综合应用。这种综合运用带来的复杂性，意味着极大的参数空间和极多的特殊情况。在这种背景下，写出来一个只在极少数情况下正常工作的代码，然后通过调参摸索可行空间，做出算例的方法将逐渐不再可行。相反，未来的物理模拟程序应当具备能在几乎任何情况下工作的能力。就算无法做到，也应当帮助研究人员定位问题，查明原因。此外，蓬勃发展的各种自适应算法（这在CFD领域已经有大量应用）将极大拓宽物理模拟程序的工作区间，使之得以处理各种复杂的物理过程。
- **多模块、多算法相互依赖。** 物理模拟领域发展至今，像不可压投影、界面追踪这样的基本问题已经得到了充分的研究，形成了若干State-of-the-Art的标准算法。如今已经不太可能需要像stable-fluids一样，把投影这样的基本算法实现作为研究贡献之一。几乎可以肯定，这些基础的标准算法会在长期内保持不变，始终拥有一席之地。但以对算法和物理的深刻理解为基础，对这些基础模块的精准选择和恰当组织，将成为物理模拟创新的重点和难点，和人工智能研究领域的最新进展类似。而各个基础模块之间，又存在着隐含的逻辑关系，如何正确把握这种逻辑关系，对研究人员提出了极高的要求。因此，我们的代码必须具备极强的健壮性和极高的抽象水平，使得研究人员可以像用`PyTorch`搭建人工智能系统一样，轻松搭建出高度复杂的模拟程序，并快速进行修改和扩展。

## 当前的问题

未来的State-of-the-Art模拟程序将不再是单个天才程序员能在短时间内完成的toy project，而是协作良好、拥有充分后勤保障的开发团队经过严密设计后实施的大型工程，这必然要求我们从软件工程的高度进行严谨的程序设计。而我们目前使用的SimpleX版本并未达到这一标准，在使用过程中，我们发现了它的若干问题。下面列举各项目的主要情况和对代码库的主要意见。

### 北大方面的意见

- CPX solver收敛速度过慢，无法接受。`parallel`的代码很难看懂。
- Repo体积过大。由于文件历史的遗留问题，这个repo已经不能用了。
- 建议仿照Eigen和PhysX，把所有内容放进一个namespace中，以方便扩展。
- 目录设计不合理，需要重新设计。相同功能的代码出现在了不同地方，导致难以整理出一个“最小可工作集”。例如：`fluid_euler`的代码散落在`proj/fluid_euler`,`common_cpx`,`common`,`poisson_cpx`,`physics`各处。建议按照功能进行分类，并保持整理出最小工作集的能力。
- 建议使用git submodule或git subtree理清代码结构。
- `physics`目录下混杂了固体、流体、SPH、PIC/FLIP等算法，还有部分legacy code，建议按照功能拆分，并进行清理。
- 各模块过度OOP化。例如projection、隐式表面张力、interpolation、levelset等均单独成工作类，这是无必要的java风格。建议采取类似`Eigen`的模板风格。例如，在`FluidEuler`类中指定interpolation的模板类，调用模板类中的函数进行算法操作。示例：`LevelSetLiquid<SemiLagrangian, MtxFreeMultiGridSolver, ImplicitSurfaceTensionSolver>`.再如，设一`MacCormack`类，类中实现`static advect()`，在`FluidEuler`中用模板传入`MacCormack`类，调用`MacCormack::advect()`进行对流。详见他们的`Advection.h`.
- `TypeFunc`采用冗杂的`SFINAE`设计范式，建议更新至C++20，采用`concept`替代之。
- Visual Studio 2022编译出现问题，建议修正。
- CUDA编译反复出现版本问题、链接问题，且无法根治。而北大的代码库采用`xmake`，仅需`add_files("*.cu")`一条命令就可以轻松编译CUDA代码。建议改用`xmake`。此外，当前的编译方案偏向monorepo风格，比较罕见。
- 建议采用倪星宇和李星桥2022年的新工作（尚未发表）取代原有对流方案。

### SPH Bubble-MELP-Thread系列项目情况

- 初始化和参数传递逻辑容易混乱，在代码逻辑修改后，很容易出现同一参数名含义改变、参数被overwrite、忘加新参数等问题，而且这些问题均为不会报错的隐性问题，容易导致参数无法复用，每次修改后跑出预料之外的结果，不得不频繁调参。在SPH Bubble项目的末期，我们采用参数类的方案部分解决了这一问题，因为当时不知道`json`工具。现在看来，`json`是一个更优的方案，应当切换至`json`.
- 一些基本的函数，例如微分算子，不应在simulation过程中通过从外部“穿透访问”的方法显式实现，而应该将其封装起来。当时曾怀疑这是否会导致一定的效率损失（例如，需要在方法内部多次计算$\bm{r}_{ij}$），但事后证明这种做法是值得的。如果当初决定继续采用刚接手SPH Bubble时的项目状态，即在simulator里面写一个大循环，在循环里面通过“穿透访问”计算所有的物理量，则SPH Bubble根本不可能完成，因其最终的复杂度远远超过了这个框架能处理的极限。
- 后续在MELP项目中，我们确认微分算子的封装应在数据结构这一层级完成。在这一项目中，我们将E粒子和L粒子的公共操作（例如微分算子、邻居搜索等）提取为`NAParticles`类，形成`NAParticles`-`EulerParticles`/`LagrangeParticles`的两级三元继承结构，实践证明这一设计是成功的，MELP项目并未陷入复杂代码导致的工程地狱。
- `Points`类的`ArrayPointer`实现优于宏实现。宏实现只应在操作系统级底层开发中出现，因其违背了工程代码最基本的清晰度原则。如果`Points`类仍保持宏实现，则我们不可能快速上线`Copy_Element_From`、`Join`、`Delete_Elements`、`Save_Snapshot`等重要功能，而这些功能在SPH Bubble和MELP中都起到了关键作用。
- 在开发复杂项目时，`opengl_viewer`已不堪重负，键位几乎排满。其数据格式难以解读，无法输出常量，导致对`h`的可视化是hack出来的。`opengl_viewer`的重复编译为项目造成了一定的困扰，曾反复出现由于版本不对导致某物理量看不到，怀疑程序出bug的现象。此外，一开始`opengl_viewer`无法支持单点调试功能，导致调试缺乏抓手。这后来使用颜色编码的方法解决，但当前仅支持tracking circle和线段这两种功能，经尝试，扩展功能开发难度很高，而且用户界面操作也不甚方便，难以推广。
- 在Thread项目中，采用`Polyscope`进行可视化，获得初步成功，同时采用`json`全权限配置，有潜力实现一次编译，永久使用的目标，这种方法可以有效摆脱上述的所有困扰。
- 在扩展项目时，难以保持并行度，尤其是SPH Bubble加入传统SPH部分后，程序运行速度急剧变慢，只得通过频繁优化的方式解决。在新`fluid_SPH`和Thread项目中，通过iterator-lambda方案解决了这一问题，新`fluid_SPH`达到了优于`VCL-PhysX`的效率。
- 采用Python脚本完成生成、运行、可视化的方法是成功的。详细的实验记录也较为重要。

### 主要问题总结

在上述情况的基础上，再结合SIG2022各项目情况和组内意见，现总结主要问题如下：

- **基础工具集亟待升级。** 当前代码库中的主要算法模块仍未超出Bridson书中描述的水平，只有项目主要方向能做个别提升。在图形学领域，未吸纳APIC、MLSRK、MPM等新兴算法，项目无法进行扎实的对照实验；在CFD领域，虽然有GPU solver的部分优化，但没有高精度格式和高精度算例，长期停留在toy example的层面，未能体现出GPU solver的优越性。
- **代码模块化程度低。** 有过度OOP的现象，例如`ProjectionCPX`、`RigidBody`等基础类内容过全、整合程度过高，导致难以理解，难以迅速按需开发。在马蹄涡项目中出现过初始化序列混乱导致程序崩掉的问题；Elastocap项目不得不放弃`ProjectionCPX`类的继承方案，在模拟器中直接操作`Poisson`类。事实证明，较之于继承方案，这种方法反而更为合理，有效支撑了后期的快速代码调优。
- **重复造轮子。** 缺乏稳定、高效、简洁、最小化的经典算法导致各项目都不敢依赖代码库进行开发，转而出于自保的考虑，各自维护一套独立的基础算法，造成了低水平重复建设。例如代码中曾经出现了两份重复的隐式表面张力和四份重复的刚体动力学代码，softbody里面重复添加隐式迭代算法，未抽象迭代过程。
- **添加新算法困难。** 无法模块化导致每次添加新功能都需要在理解全部流程的基础上穿插式添加。在横向上，这导致必须一次又一次地重写经典算法，例如马蹄涡无法直接复用先前代码库中已有的IBM、PIC/FLIP和`SpraySystem`，必须全部重新手写。如此不仅浪费时间，而且不得不一遍一遍地反复解决旧有代码的bug。在纵向上，这导致同一项目的后续开发要么甩不掉之前的包袱，负担过重（例如`topoopt_voronoi`），难以前进；要么不得不屡屡全盘重写（例如SPH Bubble-MELP的重写、马蹄涡两相流-自由表面-PIC/FLIP的重写），既拖慢开发速度，也频繁导致旧算法的成功算例在新算法上突然失效，又得从头调试，进度重来。
- **组内难以优势互补。** 这种穿插式开发使得多人合作难以实现，必须依赖单个程序员，拖慢开发速度。同时，项目之间缺乏互通性和互操作性，缺少跨项目协调机制，这共同导致了各项目之间毫无关联，无法推广State-of-the-Art算法，缺乏“1+1>2”的合作效应。例如在MELP已实现APIC的情况下，未能将其推广至其他网格流体项目，取代传统的PIC/FLIP方案。
- **缺乏标准化定量实验。** 代码validation只能靠肉眼观察，缺乏CFD对管流、圆柱绕流、台阶流、方腔顶盖流等经典算例的精细化定量分析，难以确认算法正确与否。例如肉眼观察未能察觉隐式表面张力的初始化错误问题；各项目反复被CG求解器的不同收敛逻辑困扰。在添加老算法时，不敢确定算法正确与否，只能一遍一遍调参，反复排查；在开发新算法时，缺乏标准化定量实验也导致无法进行横向跨算法对比、纵向算法优化对比，无法通过数学手段把握新算法特性，只能过度依赖调参大法，离CFD标准差距较大。例如SPH Bubble曾经长期被stiffness的数值设置和各物理量之间的平衡困扰。
- **缺乏入门文档和教程。** 新人无法理清代码架构，难以快速上手；组内一个人开发出来的新算法，其他人也不知道上哪找代码和接口。经典算法没有文档，导致新人无法从代码中学习，写程序靠猜、靠悟性。同时，缺乏文档也导致代码无法开源，落后于公开、共享的时代潮流。

# 更新方案总体设计

因此，我们提议对SimpleX代码库进行大规模更新，以适应2020年代新形势下对物理模拟代码的需求。综合考虑上述经验教训，更新后的代码库应当遵循以下设计规范：

#### 一、基于xmake&c++20的最新版开发环境

开发环境更新方案如下：

|模块    | 当前方案 | 计划方案 |升级原因|
| :---  | :------ | :---     |:------|
|C++    |c++17    |c++20     |更好的函数式支持，尝试使用module属性加快编译，使用concept方案取代`TypeFunc.h`|
|CUDA   |CUDA11.1 |CUDA11.4  |随CUDA更新保持最新版本，争取解决编译问题|
|生成方案|CMake 3.18|xmake    |保持当前成功的脚本生成方案。xmake拥有更快的编译速度，更清晰的脚本逻辑，争取解决当前的CUDA编译不一致问题和Debug模式的bigobj问题，并让开发人员拥有理解生成脚本的能力。|
|编译方案|VS2019&Linux g++|VS2021&Linux g++|随Visual Studio进行版本更新|
|第三方库编译|CPM+系统库+代码内部库|xmake+系统库|代码中不再携带任何第三方库，所有用到的库一律用xmake固定版本下载。取消`_install`项目，在具体项目当中编译非header-only的第三方库。|
|第三方库版本|不同配置|一次升级至最新版本|各第三方库一次升级至最新版本，以获取性能改善。尤其是不再使用当前的自定义Eigen版本，改为最新版本并修正错误。然后暂时固定这些最新版本号，等待下一次SimpleX大版本更新。|


此外有一些杂项：

- 取消纯CPU支持，要求使用CUDA。
- 尽量取消各种第三方库的macro guard，在生成脚本层级解决这些问题。
- 在切换至`xmake`之后，探索解决对国内网络环境的支持，和`cell_dynamics`项目的`cuDNN`编译问题。


#### 二、基于功能分层的三级主结构

新版本模拟代码将采用三级结构：
`driver`
|
`simulator`
|
`reservoir`----`solver`
`driver`负责装配算例，`simulator`组织整个模拟算法的流程，`reservoir`存放数据，而`solver`进行数学求解。`simulator`会同时引用`reservoir`和`solver`，故后二者在同一层，但`solver`可能需要看到`reservoir`提供的数据结构。

例如：`FluidEulerDriver`----`FluidEuler`----`FaceField`/`Field`/`Poisson`.注意，现在的`fluid_euler`有`FluidEulerDriver`----`FluidEuler`----`ProjectionCPX`----`Poisson`四级结构，计划删除`ProjectionCPX`这一层，由`fluid`直接调用`Poisson`.

这种方案的内在逻辑是，`driver`层负责的是“现实场景”，属于艺术；`simulator`层负责的是模拟的算法逻辑，属于物理；`reservoir`/`solver`层负责数据结构和代数运算，属于数学。

#### 三、基于json的强制初始化

在程序层面上，取消`ParseArgs`的传参功能，使用读入`.json`文件的方法传递需要从外部传入的**全部参数** ，并据此建立标准化算例体系。

这样做的核心是，让从外部传入的`.json`配置文件全权限控制模拟过程中的所有参数，于是所有的调参就都在`json`文件的层面上发生，和模拟程序的控制流程分离。如此一来，实验中用到的全部参数就能在`json`文件当中“一目了然”，而不需要去解读叠床架屋的初始化序列，可以轻松留存、确认和对比。

在类层面上，所有类需要的信息，可以归结成三种：参数（例如某个选项的开关、某个物理常数的设置）、指针（保存的其他类的指针）、数据（例如速度场）。这个类的初始化将分成三步：
1. 构造函数，使用`json`装订**全部参数**（在非常成熟的模块里面甚至建议用`private`保护这些参数），使用参数列表装订全部指针。以此强迫用户在一个类创建的时刻给出全部参数。可以为一个类设计空的默认构造函数，以保证你不需要用`shared_ptr`形式保存它，但请保证它有且仅有一次在构造函数里初始化的机会（这么设计的原理是，类似int、double这样的基本类型也有默认构造函数，因此一个类也可以这么看）。请采用`Json::Value()`接口从`json`中读取函数值，这样传入的`json`可以省略一些参数，但`Json::Value()`将输出相关信息，并把相应的默认参数填入这个`json`，于是在初始化过后，你就可以取得模拟器的完整配置，并将其输出。最重要的是，基于这一设计，对某参数制定初始值的**唯一方式** 是在`driver`里面指定默认`json`配置（甚至写成文件存放），除此之外的一切方法都是错误的，不应使用。
2. 数据部分，从外部访问内部数据结构，装订所需的数据（例如装填速度场）。这是危险的穿透访问，因此应当尽量减少它的内容。
3. （不一定有）初始化结束，可能会需要进行一些计算作为收尾。这一部分可以设置一个接口名称，例如`InitDone()`.

这里值得特别指出的是，之前把计算`MacGrid`，分配内存放在`Initialize()`而非构造函数中的做法，一旦逻辑复杂，就极易造成混乱。而我们观察到，在已知的所有项目中，没有任何一例在使用类之前不知道网格大小的例子，也没有任何一例网格大小在运行中变化的例子。这意味着，`MacGrid`的本质不是一个**数据** ，而是一个**参数** ，在模拟器类被调用之前，它就已经存在了。因此，网格大小不应在数据部分进行处理，而应当放进`json`，在构造模拟器类的时候，即可根据传入的参数进行内存分配。除了这种初始时就能确定大小的网格数据之外，还可能有一类完全是自适应动态分配的数据（比如，在`NeighborSearcher`里面存放搜索结果的结构），这种应当在调用时自适应分配，亦不需要在数据部分进行初始化。

这种方案可以从根源上避免初始化逻辑混乱造成的全部问题，缺点仅有两条：第一是`driver`类当中的`simulator`对象必须用`shared_ptr`实现，但按照我们的设计，`driver`类本就并不应过多地穿透访问`simulator`对象中的内容，因此不会造成过重的负担。第二是在构造函数之前需要程序要解决的问题想得更明白一些。但这部分心智负担本来就不可避免，与其先构造一个类出来，然后再追加参数加以补救，还不如在一开始就想好。

#### 四、基于基类-继承类两级结构的最小模块化（待定，或模板方案）

各成熟算法应当具备：
1. 简单，符合直觉的的基础版本
2. State-of-the-Art版本

将它们写成内部最优化实现，不可进一步分割的代码。这些基础模块应当遵循“高效、最小化”的原则，能写成函数的就不写成类，类中能少存数据就少存，能不存就不存，尽一切可能减少代码中的长距依赖。如果模块有多个版本，采用基类-继承类两级结构实现对外统一，便于切换的标准化接口，故可以方便地进行对比实验。

在整个代码库设计当中，`json`全权限配置为第一优先级，因此对基础算法的选择必须是运行时的。出于这一考虑，我们拟不采取北大的Eigen式模板方案，而采用基类指针方案。


#### 五、基于iterator-lambda的数据操作

全面改用“iterator+单点函数”的设计模式。由数据结构负责iterator的实现，这是在MELP项目中验证成功的设计思路。

例如，假设有一个粒子类对象`Particles<d> ps`，其中包含`n`个粒子，希望对每个粒子执行某一操作，则我们的写法是：
```c++
ps.Exec_Each(
  [&](const int i){
    //do something
  }
);
```
实现`Particles<d>::Exec_Each()`函数使之等价于：
```c++
#pragma omp parallel for
for(int i=0;i<n;i++){
  //do something
}
```
我们将原则上全部使用这种设计（除了BFS等极少数必须串行的算法）。我们认为这是可行的，理由是全部物理过程在本质上都是一个“局部”的过程，而且每个质点的地位是均等的，没有顺序之分，因此并行操作符合这一规律。

这样做的好处包括：降低心智负担（只需要写一个局部的函数）、天然强制保障并行性、易于梳理控制流。例如，可以轻松地把单点操作的定义和调用分开，而不需要昂贵地定义函数：
```c++
auto f=[&](const int i){...};
auto g=[&](const int i){...};
ps.Exec_Each(f);
ps.Exec_Each(g);
...
```

此外值得特别指出的一点是，这种数据操作模式是前后端分离的，你可以优化`Exec_Each()`的实现而不修改整个模拟代码的语义。**这一特性是为未来把全部代码扩展至CPU/GPU双模而留下的关键接口。**

#### 六、基于CPU/GPU双模数据结构的双模代码

目前GPU模拟器方案并未完全确定，但可以肯定的是它仍将基于iterator-lambda的数据操作模式（事实上这已经在当前的CPX solver中得到了成功运用）。我们初步计划使用类似`SparseMatrixCPX`的双模数据结构方案，实现算法的CPU/GPU双模调用。但如果未来证明GPU上基础算法模块的稳定性足以支撑需求，或许也可以做纯GPU版本。

开发时应考虑到这种未来的扩展需要，进行相应设计。例如，CUDA从host调用device函数时参数只能传值，因此类似`Grid<d>`这样的类应当做得尽量小，包含尽可能少的数据。


#### 七、基于“可控崩溃”的user-proof设计

新代码应当具备一定的纠错能力，主要体现为通过在非正常工况下崩溃并给出提示的方法，提醒用户写出正确的代码。现列举几个例子：

- 使用基类-继承类结构实现类似对象的不同实现时，在基类中设置空虚函数接口，迫使每个继承类都实现相应接口，如果不实现，则编译报错。例如：`ImplicitGeometry<d>`中的`virtual real Phi(const VectorD& pos) const = 0;`.
- 在构造函数中强制传入`json`，不设默认构造函数，这样保证在创建一个类的时候，必须想好它的参数是什么。
- 在从`json j`中读取参数时，不用`j.value()`，而使用`j.at()`或者`j[]`，这样一旦缺少某个参数，代码就会报exception退出，这样提醒使用者正确地装订所有参数。
- 在代码每次迭代结束后，用`Numerical_Check()`或类似函数检查算出来的物理量是否包含`nan`或`inf`，如果错误则崩溃退出，而不是让程序静默运行，给出空白的可视化结果。

#### 八、面向开源的算例和文档体系

向包括[Pintos](https://pintos-os.org/)在内的现代软件工程看齐，建设文档体系。像投影、对流这样的极基础算法应当具备详细文档和注释，包含参考文献、数学原理、实现细节、接口逻辑、负责人等内容。

在用`json`文件取代命令行参数的背景下，以CFD标准建设经典算例体系，包含精确、可重复的实验和定量的数据分析。确定定量分析的相关工具链。

# 开发计划

计划主要分成四个部分：
- 以`xmake`编译脚本为核心的基础部分
- 以`fluid_euler`项目为核心的网格流体系统
- 以`fluid_SPH`项目为核心的粒子模拟系统
- 以`polyscope_viewer`项目为核心的可视化程序

## 基础部分

##### 采用`xmake`重新设计编译方案，具体如下：

1. 保持“一条命令生成`.sln`”的生成风格。
2. 保证生成脚本的简洁性，以确保开发人员可以轻松添加项目。
3. 使用`xmake`提供的方案集成第三方库，移除所有`simplex`自身携带的第三方库。
4. 设计灵活可靠的CUDA编译方案，解决`cell_dynamics`项目和`cpx`分离式编译不兼容、`bigobj`选项这两条“老大难”问题。
5. 各第三方库一次性升级至最新版本，相应升级代码以适配可能的新接口。移除自定义的Eigen库，剥离受其影响的`opengl_viewer`（见下）。
6. 移除现有`script`目录中的全部内容，作为legacy code在其他repo中保存。

##### 其他代码更新，具体如下：

1. 新建储存库，重新设计代码架构。
2. 移除`TypeFunc.h`，改用concept方案，使用C++20的函数式模板特性取代decltype，探索采用C++20的module模块加速编译。
3. 按照功能拆分`src/physics`，因其规模过大。删除`namesapce AuxFunc`，将其按功能拆分。
4. 整理各种系统的最小工作集合。


## 网格流体系统

重写`fluid_euler`项目，主要包含`FluidEulerDriver`-`FluidEuler`-`Poisson`/`FaceField`，以及类似的自由表面流体模拟器等。

####一阶段目标
##### 重写不可压流体求解器`FluidEuler`和带自由表面的不可压流体求解器`FluidEulerFreeSurface`，具体如下：
1. 除GPU solver外所有代码从头重写，如有必要，可重新命名。按照“最小化”原则，可能非必要的接口先不加，用到了再加（例如：`Field`的各element-wise四则运算似乎从未被用过，这种情况应先不添加，等用到了再说）。
2. 执行上述各条代码规范，尤其是从`main.cpp`开始的强制`json`参数配置方案、iterator-lambda方案。
3. 按照代码规范中的三层设计理念，取消当前的`Projection`相关类，在`FluidEuler`这样的模拟器中直接调用`Poisson`进行投影计算。较为复杂的函数（例如带interface coefficient的速度矫正）以外部公共接口的形式实现。
4. 删除`MacGrid`中的`edge_grid`等。在`Field`和`FaceField`类中直接携带相应的`Grid`和`MacGrid`（待定）。理清或删除`Grid`中的`Node`概念，因其在隐式表面张力当中引起的混乱。将这些类裁剪至适合可以直接传值给GPU的大小，以为后期CPU/GPU双模做准备。
5. 实现一个精度更高（或许高于基础模拟器），更现代化的`LevelSet`.
6. 确保算法可以有效运行，实现标准算例的量化、固化、文档化，做验证实验。

##### 修正GPU solver的代码风格，具体如下：
1. 将`CPX`项目统一整理至一个库下，更名为`LM`（待定）。不再开发`parallel`，未来计划从代码库中移除，各种矩阵求解统一集中至此模块下进行维护。
2. 修改`Poisson`的初始化，采用`Grid`而非`MacGrid`.
3. `Poisson`系列工具全面支持<T,d>（float/double，2d/3d）的模板参数。
4. 移除原`CPX`自行携带的`Grid`类，功能合并至`common`下的`Grid`/`MacGrid`.
5. 优化multigrid算法，减少迭代数，并支持非2的幂次的multigrid迭代。

####二阶段目标
##### 基于初步成熟的代码库，添加更多CFD算法和CG高级算法，具体如下：
1. 以蔡庆东为基础《计算流体力学讲义》实现SIMPLE算法、谱方法、较为简单的激波模拟（待定）。以CFD标准检验各经典算例，并将这种算例结果固化为文档。
2. 重写`RigidBody`类，统一2/3D实现，使用新代码库重新实现双向流固耦合。
3. 基于新粒子系统（见下）实现大规模海面效果。
4. 在代码中添加断点续跑的能力，并尽可能实现一个强制接口。
5. 借助这一过程，检查代码的稳定性和结构合理性，及时横向沟通，反馈调整问题。

##### 若条件成熟，实现GPU advection和其他GPU函数。借助GPU加速冲击不可压模拟的速度极限。

## 粒子模拟系统

重写`fluid_SPH`项目，主要包含`FluidSPHDriver`-`FluidSPH`-`SPHParticles`等.

#### 一阶段目标
##### 重写SPH算法，具体如下：
1. `Points`类下游全部代码从头重写（待定），如有必要，可重新命名。按照“最小化”原则，可能非必要的接口先不加，用到了再加（例如：移除`TrackerPoints`和`Particles`类，若具体项目中确有需要，慎重设计后重写）。
2. 执行上述各条代码规范，尤其是从`main.cpp`开始的强制`json`参数配置方案、iterator-lambda方案。
3. 移除当前采用的`Parameter`类配置方案，用`json`接管从`main.cpp`开始的全部参数设置。
4. 重新设计`SPHParticles`，不在其中实现`Points`中已有的iterator接口。并行iterator一律使用`Points`类中的相应接口。
5. 修复MELP中暴露出的，`Points`和各继承类无法复制的问题，并在注释中给出明确提示。
6. 统一设计`Points`继承类的二进制文件输出方案。
7. 使用新代码库实现mass density、number density、PCISPH、IISPH四种SPH算法。确保各算法均有效运行，实现标准算例的量化、固化、文档化，做验证实验。

#### 二阶段目标

##### 基于初步成熟的代码库，添加最新粒子方法，并支持网格求解器开发，具体如下：

1. 重新设计`PointSet`类，移除其中过度复杂的算子内容，以最小化的方式实现几何计算。
2. 依托新粒子系统，在新网格流体系统中实现APIC算法，如需要也可整合spray particle system。
3. 在新粒子系统中优雅地实现`melp_film`算法，作为未来`thread_film`的开发基础。
4. 研究自适应SPH算法，实现一个此类算法。
5. 实现例如MLSRK或MPM这样的现代粒子算法。
6. 在代码中添加断点续跑的能力，并尽可能实现一个强制接口。
7. 借助这一过程，检查代码的稳定性和结构合理性，及时横向沟通，反馈调整问题。

## 可视化程序

##### 完善`polyscope_viewer`项目，具体如下：
1. 不再开发，并不推荐继续使用`opengl_viewer`（但保持向前兼容），迁移至`json`全功能配置的`polyscope_viewer`.
2. 剥离`opengl_viewer`至一单独项目，不再纳入`simplex`维护。将当前的自定义`Eigen`改为最新版本，移除`simplex`的`OpenGL`支持（`Polyscope`会在生成时携带自己的`OpenGL`）。
3. 采用`json`全权限控制所有的可视化，`polyscope_viewer`项目内容简化，仅设一个driver，通过`json`配置读入可视化内容。这样各项目无需重编`polyscope_viewer`，仅需填写一个`json`文件。
4. 把网格和粒子数据结构散落各处的的输出接口集中至`RenderFunc`，在输出的同时填写上述的`json`文件，这样在项目中仅需写一次输出，即可“傻瓜式”可视化。这个逻辑设计得漂亮一些，例如只需给`polyscope_viewer`指定一个输出目录，规定这个目录下必须有一个`config.json`文件。
5. 根据需求开发，支持上述提到的所有网格和粒子系统的可视化。
6. 支持网格切片操作。
7. 研究如何实现流线图等高级可视化。研究定量分析工具链。


## 人员配置

开发计划可分为若干个完整的工作单元，每个单元应由同一人完成。工作单元之间可以并行。

一阶段的任务单元包括：
- xmake
- 其他代码调整
- 网格simulator
- GPU solver
- SPH
- Polyscope.

二阶段的任务单元包括：
- 网格部分每个算法一个单元
- 网格GPU加速一个单元
- 粒子部分每个算法一个单元
- 粒子GPU加速一个单元

一阶段共六个工作单元，二阶段代码耦合度应已降低，可每个算法并行开发。在充分并行的情况下，一阶段二阶段估计各耗时一个月左右，在此期间原代码库不进行任何有意义的更新。
